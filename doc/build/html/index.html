

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to DeepLearning’s documentation! &mdash; DeepLearning 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> DeepLearning
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to DeepLearning’s documentation!</a></li>
<li><a class="reference internal" href="#algo">Algo</a><ul>
<li><a class="reference internal" href="#module-core.algo.MLP">MLP algorithm</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">DeepLearning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to DeepLearning’s documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p>sphinx-quickstart on Mon Mar 25 21:51:56 2019.
You can adapt this file completely to your liking, but it should at least
contain the root <cite>toctree</cite> directive.</p>
<div class="section" id="welcome-to-deeplearning-s-documentation">
<h1>Welcome to DeepLearning’s documentation!<a class="headerlink" href="#welcome-to-deeplearning-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="algo">
<h1>Algo<a class="headerlink" href="#algo" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-core.algo.MLP">
<span id="mlp-algorithm"></span><h2>MLP algorithm<a class="headerlink" href="#module-core.algo.MLP" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="core.algo.MLP.AbstractMlp">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">AbstractMlp</code><span class="sig-paren">(</span><em>name='AbstractMlp'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_architecture.AbstractArchitecture</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>This class set the major core of a Multi Layer Perceptron neural network. The neural network architecture
takes as input a linear vector of input data put in a succession of fully connected layers. In the end a
last layer reduce the dimensionality of the network to match with the number of target variables to predict.</p>
<p>The abstract schema assume the child class must define the methods to set the loss function. In ths way it is simple
to enlarge this architecture to any type of problems.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>Tensor</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>Tensor</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>Tensor</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>Tensor</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>Tensor</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><em>FcLayer</em><em>]</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<em>FcLayer</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<em>AbstractLoss</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary getting all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.restore" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows to save all the Tensorflow graph and all network parameters in a folder. The methods use the
Tensorflow saver method and save all network parameters in a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.algo.MLP.MlpClassifier">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">MlpClassifier</code><span class="sig-paren">(</span><em>name='MlpClassifier'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.algo.MLP.AbstractMlp" title="core.algo.MLP.AbstractMlp"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.algo.MLP.AbstractMlp</span></code></a></p>
<p>This class allows to train a MLP for classification task. The target array must be a One Hot Vector Encoding
with dimension equal to the number of label to predict. In addition the class provide an additional methods to
predict directly the probability for each label.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>Tensor</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>Tensor</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>Tensor</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>Tensor</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>Tensor</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><em>FcLayer</em><em>]</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<em>FcLayer</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<em>AbstractLoss</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict a vector of probability for each label.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observations</em><em>, </em><em>n_inputs</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observation to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predict probabilities.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation, n_labels)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary getting all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.restore" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows to save all the Tensorflow graph and all network parameters in a folder. The methods use the
Tensorflow saver method and save all network parameters in a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.algo.MLP.MlpRegressor">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">MlpRegressor</code><span class="sig-paren">(</span><em>name='MlpRegressor'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.algo.MLP.AbstractMlp" title="core.algo.MLP.AbstractMlp"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.algo.MLP.AbstractMlp</span></code></a></p>
<p>This class allows to train a MLP for regression task. The target array must be a square matrix having one or more
objective variable to learn.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>Tensor</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>Tensor</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>Tensor</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>Tensor</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>Tensor</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><em>FcLayer</em><em>]</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<em>FcLayer</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<em>AbstractLoss</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em>) – Name of the activation function.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None. (Ex: L1 or L2)</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary getting all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows to save all the Tensorflow graph and all network parameters in a folder. The methods use the
Tensorflow saver method and save all network parameters in a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Nicolas de Rémacle

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>