

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Welcome to DeepLearning’s documentation! &mdash; DeepLearning 0.0.1 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script type="text/javascript" src="_static/jquery.js"></script>
        <script type="text/javascript" src="_static/underscore.js"></script>
        <script type="text/javascript" src="_static/doctools.js"></script>
        <script type="text/javascript" src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="#" class="icon icon-home"> DeepLearning
          

          
          </a>

          
            
            
              <div class="version">
                0.0.1
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#">Welcome to DeepLearning’s documentation!</a></li>
<li><a class="reference internal" href="#algo">Algo</a><ul>
<li><a class="reference internal" href="#module-core.algo.MLP">MLP Algorithm</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deep-learning">Deep Learning</a><ul>
<li><a class="reference internal" href="#module-core.deep_learning.abstract_architecture">Abstract Architecture</a></li>
<li><a class="reference internal" href="#module-core.deep_learning.abstract_operator">Abstract Operator</a></li>
<li><a class="reference internal" href="#module-core.deep_learning.layer">Layer</a></li>
</ul>
</li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">DeepLearning</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li>Welcome to DeepLearning’s documentation!</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p>sphinx-quickstart on Mon Mar 25 21:51:56 2019.
You can adapt this file completely to your liking, but it should at least
contain the root <cite>toctree</cite> directive.</p>
<div class="section" id="welcome-to-deeplearning-s-documentation">
<h1>Welcome to DeepLearning’s documentation!<a class="headerlink" href="#welcome-to-deeplearning-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="algo">
<h1>Algo<a class="headerlink" href="#algo" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-core.algo.MLP">
<span id="mlp-algorithm"></span><h2>MLP Algorithm<a class="headerlink" href="#module-core.algo.MLP" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="core.algo.MLP.AbstractMlp">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">AbstractMlp</code><span class="sig-paren">(</span><em>name='AbstractMlp'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_architecture.AbstractArchitecture" title="core.deep_learning.abstract_architecture.AbstractArchitecture"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_architecture.AbstractArchitecture</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>This class set the major core of a Multi Layer Perceptron neural network. The neural network architecture
takes as input a linear vector of input data put in a succession of fully connected layers. In the end a
last layer reduce the dimensionality of the network to match with the number of target variables to predict.</p>
<p>The abstract schema assume the child class must define the methods to set the loss function. In ths way it is simple
to enlarge this architecture to any type of problems.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Tuple</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>str</em><em>, </em><em>None</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_param</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>]</em><em>, </em><em>None</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>, </em><em>None</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLoss" title="core.deep_learning.abstract_operator.AbstractLoss"><em>AbstractLoss</em></a><em>, </em><em>None</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_param</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>optimizer_name</strong> (<em>str</em>) – Name of the optimization method use to train the network.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary having all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.AbstractMlp.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.AbstractMlp.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all Tensorflow graph and all network parameters in a folder. The methods use the Tensorflow saver method
and save all network parameters inside a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by a ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.algo.MLP.MlpClassifier">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">MlpClassifier</code><span class="sig-paren">(</span><em>name='MlpClassifier'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.algo.MLP.AbstractMlp" title="core.algo.MLP.AbstractMlp"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.algo.MLP.AbstractMlp</span></code></a></p>
<p>This class allows to train a MLP for classification task. The target array must be a One Hot Vector Encoding
with dimension equal to the number of label to predict. In addition the class provide an additional methods to
predict directly the probability for each label.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function are used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>str</em><em>, </em><em>None</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_params</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>]</em><em>, </em><em>None</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>, </em><em>None</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLoss" title="core.deep_learning.abstract_operator.AbstractLoss"><em>AbstractLoss</em></a><em>, </em><em>None</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.predict_proba">
<code class="descname">predict_proba</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.predict_proba" title="Permalink to this definition">¶</a></dt>
<dd><p>Predict a vector of probability for each label.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observations</em><em>, </em><em>n_inputs</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observation to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predicted probabilities.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation, n_labels)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_param</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>optimizer_name</strong> (<em>str</em>) – Name of the optimization method use to train the network.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary having all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpClassifier.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpClassifier.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all Tensorflow graph and all network parameters in a folder. The methods use the Tensorflow saver method
and save all network parameters inside a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by a ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.algo.MLP.MlpRegressor">
<em class="property">class </em><code class="descclassname">core.algo.MLP.</code><code class="descname">MlpRegressor</code><span class="sig-paren">(</span><em>name='MlpRegressor'</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.algo.MLP.AbstractMlp" title="core.algo.MLP.AbstractMlp"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.algo.MLP.AbstractMlp</span></code></a></p>
<p>This class allows to train a MLP for regression task. The target array must be a square matrix having one or more
objective variable to learn.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting can be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>layer_size</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em><em>, </em><em>None</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>str</em><em>, </em><em>None</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_param</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>x</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Input tensor of the network.</li>
<li><strong>y</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor containing all True target variable to predict.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Output of the network.</li>
<li><strong>loss</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Loss function optimized to train the MLP.</li>
<li><strong>y_pred</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Prediction tensor.</li>
<li><strong>l_fc</strong> (<em>List</em><em>[</em><a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>]</em><em>, </em><em>None</em>) – List containing all fully connected layer objects.</li>
<li><strong>l_output</strong> (<a class="reference internal" href="#core.deep_learning.layer.FcLayer" title="core.deep_learning.layer.FcLayer"><em>FcLayer</em></a><em>, </em><em>None</em>) – Final layer for network output reduction.</li>
<li><strong>l_loss</strong> (<a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLoss" title="core.deep_learning.abstract_operator.AbstractLoss"><em>AbstractLoss</em></a><em>, </em><em>None</em>) – Loss layer object.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>layer_size</em>, <em>input_dim</em>, <em>output_dim</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>decay=0.999</em>, <em>decay_renorm=False</em>, <em>epsilon=0.001</em>, <em>penalization_rate=0.0</em>, <em>penalization_type=None</em>, <em>optimizer_name='Adam'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the network architecture.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>layer_size</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em>) – Number of neurons for each fully connected step.</li>
<li><strong>input_dim</strong> (<em>int</em>) – Number of input data.</li>
<li><strong>output_dim</strong> (<em>int</em>) – Number of target variable to predict.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>float</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>penalization_rate</strong> (<em>float</em>) – Penalization rate if regularization is used.</li>
<li><strong>penalization_type</strong> (<em>None</em><em>, </em><em>str</em>) – Indicates the type of penalization to use if not None.</li>
<li><strong>law_name</strong> (<em>str</em>) – Law of the random law to used. Must be “normal” for normal law or “uniform” for uniform law.</li>
<li><strong>law_param</strong> (<em>float</em>) – Law parameters dependent to the initialised law choose. If uniform, all tensor
elements are initialized using U(-law_params, law_params) and if normal all parameters are initialized
using a N(0, law_parameters).</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>optimizer_name</strong> (<em>str</em>) – Name of the optimization method use to train the network.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>x</em>, <em>y</em>, <em>n_epoch=1</em>, <em>batch_size=10</em>, <em>learning_rate=0.001</em>, <em>rmax=3.0</em>, <em>rmin=0.33</em>, <em>dmax=5</em>, <em>verbose=True</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Fit the MLP <cite>n_epoch</cite> using the <cite>x</cite> and <cite>y</cite> array of observations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>y</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>output_dim</em><em>)</em>) – Array of target which must have a dimension equal to output_dim.</li>
<li><strong>n_epoch</strong> (<em>int</em>) – Number of epochs to train the neural network.</li>
<li><strong>batch_size</strong> (<em>int</em>) – Number of observations to used for each backpropagation step.</li>
<li><strong>learning_rate</strong> (<em>float</em>) – Learning rate use for gradient descent methodologies.</li>
<li><strong>rmin</strong> (<em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>verbose</strong> (<em>bool</em>) – If True print the value of the loss function after each epoch.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get a dictionary containing all network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary having all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>x</em>, <em>batch_size=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make predictions using the <cite>x</cite> array. If <cite>batch_size</cite> is not None predictions are proceed by mini-batch.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>array with shape</em><em> (</em><em>n_observation</em><em>, </em><em>input_dim</em><em>)</em>) – Array of input which must have a dimension equal to input_dim.</li>
<li><strong>batch_size</strong> (<em>int</em><em>, </em><em>None</em>) – Number of observations to used for each prediction step. If None predict all label using a single step.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Array of predictions</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array with shape (n_observation,)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.algo.MLP.MlpRegressor.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.algo.MLP.MlpRegressor.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all Tensorflow graph and all network parameters in a folder. The methods use the Tensorflow saver method
and save all network parameters inside a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by a ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="deep-learning">
<h1>Deep Learning<a class="headerlink" href="#deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-core.deep_learning.abstract_architecture">
<span id="abstract-architecture"></span><h2>Abstract Architecture<a class="headerlink" href="#module-core.deep_learning.abstract_architecture" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture">
<em class="property">class </em><code class="descclassname">core.deep_learning.abstract_architecture.</code><code class="descname">AbstractArchitecture</code><span class="sig-paren">(</span><em>name</em>, <em>use_gpu=False</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Cortex for any deep learning architecture. The class initialization step allows to configure the Tensorflow
interface to use GPU computation for example. In addition this abstract level implement general usage such the
saving and the restoration methods. It provide also some always use methods such the minimizer or placeholder
setting.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>name</strong> (<em>str</em>) – Name of the network.</li>
<li><strong>use_gpu</strong> (<em>bool</em>) – If true train the network on a single GPU otherwise used all cpu. Parallelism setting will be improve with
future version.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>graph</strong> (<em>tf.Graph</em>) – Graph of the network useful to isolate network environment if many architecture object are used.</li>
<li><strong>sess</strong> (<em>tf.Session</em>) – Tensorflow session mostly used to make Tensor computations.</li>
<li><strong>optimizer_name</strong> (<em>str</em>) – Name of the optimizer to use. Could become child attribute in future versions</li>
<li><strong>learning_curve</strong> (<em>list</em>) – A list containing the value of the loss after each training step.</li>
<li><strong>learning_rate</strong> (<em>tf.Tensor</em>) – Learnig rate tensor for optimization.</li>
<li><strong>keep_proba_tensor</strong> (<em>tf.Tensor</em>) – Tensor for dropout methods.</li>
<li><strong>is_training</strong> (<em>tf.Tensor</em>) – Tensor indicating if data are used for training or to make prediction. Useful for batch normalization.</li>
<li><strong>dmax</strong> (<em>tf.Tensor</em>) – Tensor used to clip the batch renormalization scale parameter.</li>
<li><strong>rmin</strong> (<em>tf.Tensor</em>) – Lower bound used to clip the batch renormalization shift parameter.</li>
<li><strong>rmax</strong> (<em>tf.Tensor</em>) – Upper bound used to clip the batch renormalization shift parameter.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._set_session">
<code class="descname">_set_session</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._set_session" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the Tensorflow Session and return the session object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Tensorflow session object.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">tf.Session</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._build">
<code class="descname">_build</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._build" title="Permalink to this definition">¶</a></dt>
<dd><p>Can be called to instance often used deep learning tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Methods to build the Neural Network cortex. In a first time all network arguments are update into the
dict class then the graph is build and all variable initialized. The name scope is used to ensure a valid
tensor naming: network_name/operator_name/tensor. The ‘/’ allows to insure tensorflow used a good network
name when the build is recalled.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>kwargs</strong> (<em>Any</em>) – Neural network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.fit">
<code class="descname">fit</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>The fit methods to train the neural network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>kwargs</strong> (<em>Any</em>) – Input array and fit parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.predict">
<code class="descname">predict</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Make a prediction using input arrays with shape (n_observations, …).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>kwargs</strong> (<em>Any</em>) – Input array and predict parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Array of prediction.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">array with shape (n_observations, ..)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.save">
<code class="descname">save</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.save" title="Permalink to this definition">¶</a></dt>
<dd><p>Save all Tensorflow graph and all network parameters in a folder. The methods use the Tensorflow saver method
and save all network parameters inside a pickle file.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by a ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.get_params">
<code class="descname">get_params</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Return all network parameters. The child class can call the parents get_params methods to get all global
network parameters.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Dictionary having all network parameters.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">Dict[str, Any]</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._check_and_restore">
<code class="descname">_check_and_restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._check_and_restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if all folder and file exist and restore the graph and all class attributes then.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>path_folder</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore the graph by activating all restoration process. The RESTORE environment is first set to True
to activate all class restoration methods when build is call. If a failure append, set the RESTORE
environment variable to False and raise the error.</p>
<p>Use the scope name to use the algorithm name for all tensor and operation of the network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>path_folder</strong> (<em>str</em>) – Path of the folder where the network is saved. It must ended by ‘/’.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._get_optimizer">
<code class="descname">_get_optimizer</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._get_optimizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the valid optimizer.
TODO: Move into tf_utils.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">Tensorflow optimizer object.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">tf.train.Optimizer</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._minimize">
<code class="descname">_minimize</code><span class="sig-paren">(</span><em>f</em>, <em>name='optimizer'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._minimize" title="Permalink to this definition">¶</a></dt>
<dd><p>Return an optimizer which minimize a tensor f. Add all parameters store in the UPDATE_OPS such that all moving
mean and variance parameters of a batch normalization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>f</strong> (<em>tf.Tensor</em>) – function to minimize.</li>
<li><strong>name</strong> (<em>str</em>) – name of the tensor optimizer.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Tensorflow optimizer object.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.train.Optimizer</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_architecture.AbstractArchitecture._placeholder">
<code class="descname">_placeholder</code><span class="sig-paren">(</span><em>dtype</em>, <em>shape</em>, <em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_architecture.AbstractArchitecture._placeholder" title="Permalink to this definition">¶</a></dt>
<dd><p>Set or restore a placeholder.
TODO: Move into tf_utils.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dtype</strong> (<em>tf.DType</em>) – Type of the placeholder.</li>
<li><strong>shape</strong> (<em>Sequence</em><em>[</em><em>int</em><em>]</em><em>, </em><em>None</em>) – Size of the placeholder.</li>
<li><strong>name</strong> (<em>str</em>) – name of the placeholder.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Tensorflow placeholder object.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.placeholder</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-core.deep_learning.abstract_operator">
<span id="abstract-operator"></span><h2>Abstract Operator<a class="headerlink" href="#module-core.deep_learning.abstract_operator" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="core.deep_learning.abstract_operator.AbstractOperator">
<em class="property">class </em><code class="descclassname">core.deep_learning.abstract_operator.</code><code class="descname">AbstractOperator</code><span class="sig-paren">(</span><em>name</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractOperator" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>Abstract class implementing the build or restore operator concept.</p>
<p>The main objective of the framework is to build state of the art deep learning operator where the code
used to deploy Tensorflow operators is exactly the same than the code used to restore them. Today the framework
allows to restore a complete graph using a code different to the code used to initialized it. To tackle this problem
the AbstractOperator define an abstract level which used the build methods like a way to build a first time
a part of the graph and also a way to restore it. If the build methods is call when the RESTORE environment variable
is True then all operators of a given graph used to build an algorithm run their restore methods when their build
methods is called in order to correctly restore the Tensorflow operator. However, if the RESTORE variable
is False then the build methods called a _build methods which must deploy the part of the graph.</p>
<p>The main goal is to become more focus about the operator optimization allowing to develop faster more efficient and
robust algorithms.</p>
<p>In addition the class used the graph scope level to assign a clean name for all tensor created.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>str</em>) – Name of the operator. Useful to flag all operator attribute in the Tensorflow graph.</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Union</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">core.deep_learning.abstract_operator</span> <span class="k">import</span> <span class="n">AbstractOperator</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">core.deep_learning.tf_utils</span> <span class="k">import</span> <span class="n">get_tf_tensor</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">AbstractOperator</span><span class="p">):</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x_out</span> <span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">_build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span> <span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;x_out&quot;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x</span> <span class="o">=</span> <span class="n">get_tf_tensor</span><span class="p">(</span><span class="s2">&quot;x&quot;</span><span class="p">)</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x_out</span> <span class="o">=</span> <span class="n">get_tf_tensor</span><span class="p">(</span><span class="s2">&quot;x_out&quot;</span><span class="p">)</span>
<span class="gp">...</span>
</pre></div>
</div>
<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractOperator.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractOperator.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build or restore an existing operator using the valid scope name. If the environment variable is set to True the
<cite>restore</cite> class method is called whereas if False the <cite>_build private</cite> class method is called.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<code class="xref py py-data docutils literal notranslate"><span class="pre">Any</span></code>) – Any
Build arguments defined by the child class.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractOperator._build">
<code class="descname">_build</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractOperator._build" title="Permalink to this definition">¶</a></dt>
<dd><p>Private method which must contains the main code to build the operator.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>Any</em>) – Build argument which must be define by the child class.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractOperator.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractOperator.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore properly all class attribute. Use <cite>core.deep_learning.tf_utils.get_tf_tensor</cite> to safely restore a
tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>Any</em>) – Restore argument which must be define by the child class.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.deep_learning.abstract_operator.AbstractLayer">
<em class="property">class </em><code class="descclassname">core.deep_learning.abstract_operator.</code><code class="descname">AbstractLayer</code><span class="sig-paren">(</span><em>act_funct=None</em>, <em>keep_proba=None</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>is_training=None</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>decay=0.99</em>, <em>epsilon=0.001</em>, <em>decay_renorm=0.001</em>, <em>rmin=0.33</em>, <em>rmax=3</em>, <em>dmax=5</em>, <em>name=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractOperator" title="core.deep_learning.abstract_operator.AbstractOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_operator.AbstractOperator</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>This class set a cortex for the implementation of a layer.</p>
<p>An abstract layer is a cortex for any kind of deep learning layers. It inherit of all AbstractOperator properties
and allows layers to access to often used deep learning methods such that the activation_function, batch
normalization or dropout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name for the activation function to use.
If None, no function is applied.</li>
<li><strong>keep_proba</strong> (<em>tf.Tensor</em><em>, </em><em>float</em><em>, </em><em>None</em>) – Probability to keep a neuron activate during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization after the _operator methods.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True used the batch renormalization after the _operator methods.</li>
<li><strong>is_training</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor indicating if data are used for training or to make prediction. Useful for batch normalization.</li>
<li><strong>law_name</strong> (<em>str</em>) – Name of the law to use to initialized Variable.</li>
<li><strong>law_param</strong> (<em>float</em>) – Parameter of the law used to initialized weight. This parameter is law_name dependent.</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>rmin</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
<li><strong>name</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the operator to flag it in the Tensorflow graph.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>x</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Input tensor of the operator.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Output of the operator.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">core.deep_learning.abstract_operator</span> <span class="k">import</span> <span class="n">AbstractLayer</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">Sqrt</span><span class="p">(</span><span class="n">AbstractLayer</span><span class="p">):</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
<span class="gp">... </span>       <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">_operator</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">x_out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">x</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>
<span class="gp">...</span>
</pre></div>
</div>
<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._build">
<code class="descname">_build</code><span class="sig-paren">(</span><em>x</em>, <em>*init_args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._build" title="Permalink to this definition">¶</a></dt>
<dd><p>Regarding parameters set by the child class, the build methods executes step by step the following methods:</p>
<blockquote>
<div><ol class="arabic simple">
<li>store and identify the operator input</li>
<li>check if the input tensor satisfy all class requirement</li>
<li>initialize variable tensor if needed</li>
<li>execute the _operator methods which must set self.x_out using self.x</li>
<li>apply the batch normalization or the batch renormalization</li>
<li>use an activation function if needed</li>
<li>apply dropout if needed</li>
<li>identify the output</li>
</ol>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>x</strong> (<em>tf.Tensor</em>) – Input tensor for the layer.</li>
<li><strong>init_args</strong> (<em>Any</em>) – Argument for the weight initialization. Could be an array to initialize Variable values.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the build methods of the parent class which run the build or restore process. The output of the operator
is return allowing to chain operators. This method is an abstract method to oblige the implementation of all
arguments specification.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>args</strong> (<em>Any</em>) – Argument for the _build methods which must contain at least the input operator.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Output operator Tensor.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._check_input">
<code class="descname">_check_input</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._check_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply test on the layer input.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._init_variable">
<code class="descname">_init_variable</code><span class="sig-paren">(</span><em>*init_args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._init_variable" title="Permalink to this definition">¶</a></dt>
<dd><p>Allows to initialized all layer Variable tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._operator">
<code class="descname">_operator</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._operator" title="Permalink to this definition">¶</a></dt>
<dd><p>The main implementation of the operator must be set here. This methods assume the operator takes as input
the class attribute self.x and write it output on the attribute self.x_out.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Method which restore all class tensor given the operator name and the current graph. The parent class can be
call to restore standard input and output tensor avoiding code repetition.
Use <cite>core.deep_learning.tf_utils.get_tf_tensor</cite> to safely restore a tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._apply_dropout">
<code class="descname">_apply_dropout</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._apply_dropout" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply the dropout operator on the output attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._apply_batch_norm">
<code class="descname">_apply_batch_norm</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._apply_batch_norm" title="Permalink to this definition">¶</a></dt>
<dd><p>Apply batch normalization on the output class attribute before the activation function in order to scale the
layer avoiding vanishing gradient problems. For a mini-batch with size <cite>B</cite> the normalization is:</p>
<blockquote>
<div><ul class="simple">
<li><img class="math" src="_images/math/ce8014a08973ddf4419cb5210a9824bc7bc3a18a.png" alt="\mu = \frac{1}{B} \sum_{i=1}^{B} y_{i}"/></li>
<li><img class="math" src="_images/math/071b635567da080a850f6fa68023804a5ed6cc32.png" alt="\sigma = \frac{1}{B} \sum_{i=1}^{B} (y_{i} - \mu)^2"/></li>
<li><img class="math" src="_images/math/c4c8aa53d5465ddade207eacbb4cb03e0eda45ec.png" alt="\hat{y} = \frac{y - \mu}{\sqrt{\sigma + \epsilon}} \times \gamma + \beta"/></li>
</ul>
</div></blockquote>
<p>With <img class="math" src="_images/math/3666981dc77862de77b6ecfcb64aad59b425cbaf.png" alt="\gamma"/> and <img class="math" src="_images/math/410a9d0df9c135dd73b269cba7ef04dcfd932b1f.png" alt="\beta"/> to parameters learn during training to avoid the network to rebuild the
initial value. The parameter epsilon is set to avoid infinity problem when dividing by the layer standard
deviation. For inference, the two momentum are learnt online during training using a moving average depending
to the decay parameter:</p>
<blockquote>
<div><ul class="simple">
<li><img class="math" src="_images/math/29cb67697518145599afdd25aa15357f59563fdb.png" alt="\mu_{t} = \mu_{t-1} \times decay + (1 - decay) \times \mu"/></li>
<li><img class="math" src="_images/math/854eff6e7349bb5dba20261154ea5ea8b7b3fc87.png" alt="\sigma_{t} = \sigma_{t-1}  \times decay +  (1 - decay) \times \sigma"/></li>
</ul>
</div></blockquote>
<p>These last parameters are used to normalize inference data. However, using this process the network
normalization computation is not the same between train and inference sample. Batch renormalization methods
allows to tackle this problem by adding an intermediary normalization step. For a mini-batch with size <cite>B</cite>
the renormalization during training becomes:</p>
<blockquote>
<div><ul class="simple">
<li><img class="math" src="_images/math/cfb19f4f37e7b8b4b00e0139b1a6f274b6c8c501.png" alt="r = Clip_{(rmin, rmax)} (\frac{\sigma}{\sigma_{t}})"/></li>
<li><img class="math" src="_images/math/48f7388dcda91d45695a94710abf6a43998017ac.png" alt="d = Clip_{(-dmax, dmax)} (\frac{\mu - \mu_{t}}{\sigma_{t}})"/></li>
<li><img class="math" src="_images/math/8e937f3c7c17081edb21a3700d4f064e9395bb9e.png" alt="\hat{y} = (\frac{y - \mu}{\sqrt{\sigma + \epsilon}} \times r + d)  \times \gamma + \beta"/></li>
</ul>
</div></blockquote>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Data format must be ‘NC’ or ‘NHWC’.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLayer._apply_act_funct">
<code class="descname">_apply_act_funct</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLayer._apply_act_funct" title="Permalink to this definition">¶</a></dt>
<dd><p>Use an activation function on the output class attribute.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.deep_learning.abstract_operator.AbstractLoss">
<em class="property">class </em><code class="descclassname">core.deep_learning.abstract_operator.</code><code class="descname">AbstractLoss</code><span class="sig-paren">(</span><em>penalization_rate=0.5</em>, <em>penalization_type=None</em>, <em>name='loss'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractOperator" title="core.deep_learning.abstract_operator.AbstractOperator"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_operator.AbstractOperator</span></code></a>, <code class="xref py py-class docutils literal notranslate"><span class="pre">abc.ABC</span></code></p>
<p>This class set a cortex for the implementation of a loss.</p>
<p>The class takes inherite all property of the AbstractOperator class to use the restore or build operator process.
The loss can be view as a graph operator taking as input a target tensor y and and a network prediction tensor
x_out. It can use a last transformation or return directly the algorithm prediction y_pred. The output is a loss
tensor representing the final function to minimize to train the algorithm. In addition regularization function can
be applied on a list of weight transforming the final function to optimize.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>penalization_rate</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – Penalization rate for the weight regularization.</li>
<li><strong>penalization_type</strong> (<em>str</em>) – Specify the type of regularization to applied on weight.</li>
<li><strong>name</strong> (<em>str</em>) – Name of the loss operator</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>y</strong> (<em>tf.Tensor</em>) – Placeholder containing all target variable to learn.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em>) – Output of the network.</li>
<li><strong>y_pred</strong> (<em>tf.Tensor</em>) – Final prediction return by the network.</li>
<li><strong>loss</strong> (<em>tf.Tensor</em>) – loss function of the network</li>
<li><strong>loss_opt</strong> (<em>tf.Tensor</em>) – Final loss function to optimize representing the sum of the loss with all regularization parts.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<p class="rubric">Examples</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">typing</span> <span class="k">import</span> <span class="n">Union</span><span class="p">,</span> <span class="n">Sequence</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">core.deep_learning.abstract_operator</span> <span class="k">import</span> <span class="n">AbstractLoss</span>
<span class="go">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">class</span> <span class="nc">MAE</span><span class="p">(</span><span class="n">AbstractLoss</span><span class="p">):</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">penalization_rate</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">float</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">penalization_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span><span class="p">):</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">penalization_rate</span><span class="p">,</span> <span class="n">penalization_type</span><span class="p">,</span> <span class="s2">&quot;mae&quot;</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_out</span> <span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">y</span> <span class="p">:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">list_weight</span> <span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">Variable</span><span class="p">]</span><span class="o">=</span><span class="p">())</span><span class="o">-&gt;</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
<span class="gp">... </span>       <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">x_out</span><span class="p">,</span> <span class="n">list_weight</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">_set_predict</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="bp">self</span><span class="o">.</span><span class="n">y_predict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_network</span>
<span class="gp">...</span>
<span class="gp">... </span>    <span class="k">def</span> <span class="nf">_set_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>      <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">y_pred</span><span class="p">)))</span>
<span class="gp">...</span>
<span class="gp">... </span>   <span class="k">def</span> <span class="nf">restore</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="gp">... </span>        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">restore</span><span class="p">()</span>
<span class="gp">...</span>
</pre></div>
</div>
<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>*args</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Use the parents build methods to use the restore or _build process. The build output the loss to optimize and
the loss function independent of any regularization.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>kwargs</strong> (<em>Any</em>) – Key arguments for the _build methods which must be define by the child class.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><code class="xref py py-class docutils literal notranslate"><span class="pre">Tuple</span></code>[<code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>, <code class="xref py py-class docutils literal notranslate"><span class="pre">Tensor</span></code>]</td>
</tr>
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><ul class="simple">
<li><em>tf.Tensor</em> – Loss tensor to optimize.</li>
<li><em>tf.Tensor</em> – Loss tensor without any regularization terms.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss.check_input">
<code class="descname">check_input</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss.check_input" title="Permalink to this definition">¶</a></dt>
<dd><p>Check all input tensor types</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss._set_loss">
<code class="descname">_set_loss</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss._set_loss" title="Permalink to this definition">¶</a></dt>
<dd><p>Abstract methods which set the loss tensor using the y_pred and y tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss._set_predict">
<code class="descname">_set_predict</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss._set_predict" title="Permalink to this definition">¶</a></dt>
<dd><p>Methods which must set the prediction tensor y_pred use to compute prediction.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss._build">
<code class="descname">_build</code><span class="sig-paren">(</span><em>y</em>, <em>x_out</em>, <em>weights=()</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss._build" title="Permalink to this definition">¶</a></dt>
<dd><p>The _build method executes the following steps:</p>
<blockquote>
<div><ol class="arabic simple">
<li>set all attributes</li>
<li>check the format of all tensor input</li>
<li>set the loss function</li>
<li>set the predict tensor</li>
<li>if the weights sequence is not empty, add a regularization term to the loss function</li>
<li>identify all output tensor</li>
</ol>
</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>y</strong> (<em>tf.Tensor</em>) – Tensor which contains all objective variable the algorithm learn.</li>
<li><strong>x_out</strong> (<em>tf.Tensor</em>) – Output of the network which must be transform to obtain the final prediction.</li>
<li><strong>weights</strong> (<em>Sequence</em><em>[</em><em>tf.Variable</em><em>]</em>) – A series of weighs tensor which must be subject to a regularization function.</li>
<li><strong>kwargs</strong> (<em>Any</em>) – Additional parameters which can be define by the child class.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last"><code class="docutils literal notranslate"><span class="pre">None</span></code></p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore all loss tensor attributes.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.abstract_operator.AbstractLoss._compute_penalization">
<code class="descname">_compute_penalization</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.abstract_operator.AbstractLoss._compute_penalization" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the penalty to apply to a list of weight tensor. TODO: Move this function into tf_utils.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-core.deep_learning.layer">
<span id="layer"></span><h2>Layer<a class="headerlink" href="#module-core.deep_learning.layer" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="core.deep_learning.layer.FcLayer">
<em class="property">class </em><code class="descclassname">core.deep_learning.layer.</code><code class="descname">FcLayer</code><span class="sig-paren">(</span><em>size</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>is_training=None</em>, <em>name='fc'</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>decay=0.99</em>, <em>epsilon=0.001</em>, <em>decay_renorm=0.99</em>, <em>rmin=0.33</em>, <em>rmax=3</em>, <em>dmax=5</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.FcLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLayer" title="core.deep_learning.abstract_operator.AbstractLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_operator.AbstractLayer</span></code></a></p>
<p>Use a fully connected layer having a number of neurons equal to the <cite>size</cite> parameters. A neurons is a
function making a linear transformation on a set of input and output a single output value bounded by an
activation function: <img class="math" src="_images/math/3a1961e9d1c7c49f23c872aade3a8c75958c07be.png" alt="y = \sigma(b + W \times x)"/></p>
<p>To prevent overfitting the class allows the used of state of the art deep learning regularization method: batch
normalization, batch renormalization and dropout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> (<em>int</em>) – Number of neurons of the layer.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – name of the activation function to use. If None, no activation function is used.</li>
<li><strong>keep_proba</strong> (<em>(</em><em>tf.Tensor</em><em>, </em><em>float</em><em>)</em>) – Probability to keep a neuron activated during training.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization method.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – If True apply the batch renormalization method.</li>
<li><strong>is_training</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor indicating if data are used for training or to make prediction. Useful for batch normalization.</li>
<li><strong>name</strong> (<em>str</em>) – Name of the layer.</li>
<li><strong>law_name</strong> (<em>str</em>) – Name of the law to use to initialized weights and biases.</li>
<li><strong>law_param</strong> (<em>float</em>) – Parameter of the law used to initialized weight. This parameter is law_name dependent.</li>
<li><strong>decay</strong> (<em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>rmin</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>tf.Tensor</em><em> or </em><em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> (<em>tf.Variable with size</em><em> (</em><em>input_dim</em><em>, </em><em>size</em><em>)</em>) – Weight of the layer. Must be learnt.</li>
<li><strong>b</strong> (<em>tf.Variable with size</em><em> (</em><em>size</em><em>,</em><em>)</em>) – Bias of the layer. Must be learnt.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.deep_learning.layer.FcLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>x</em>, <em>w_init=None</em>, <em>b_init=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.FcLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Call the build parents method and return the layer output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>tf.Tensor</em>) – Input array with shape (n_observation, size)</li>
<li><strong>w_init</strong> (<em>np.array with shape</em><em> (</em><em>input_dim</em><em>, </em><em>size</em><em>)</em><em>, </em><em>None</em>) – Matrix to initialize the weight variable.</li>
<li><strong>b_init</strong> (<em>np.array with shape</em><em> (</em><em>size</em><em>,</em><em>)</em><em>, </em><em>None</em>) – Matrix of bias to initialize bias.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Layer output Tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.layer.FcLayer.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.FcLayer.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore input/output tensor and all layer variables.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.deep_learning.layer.Conv1dLayer">
<em class="property">class </em><code class="descclassname">core.deep_learning.layer.</code><code class="descname">Conv1dLayer</code><span class="sig-paren">(</span><em>filter_width</em>, <em>n_filters</em>, <em>stride=1</em>, <em>padding='VALID'</em>, <em>add_bias=True</em>, <em>act_funct='relu'</em>, <em>keep_proba=1.0</em>, <em>batch_norm=False</em>, <em>batch_renorm=False</em>, <em>is_training=None</em>, <em>name='conv'</em>, <em>law_name='uniform'</em>, <em>law_param=0.1</em>, <em>decay=0.99</em>, <em>epsilon=0.001</em>, <em>decay_renorm=0.99</em>, <em>rmin=0.33</em>, <em>rmax=3</em>, <em>dmax=5</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.Conv1dLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLayer" title="core.deep_learning.abstract_operator.AbstractLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_operator.AbstractLayer</span></code></a></p>
<p>Build a 1d convolution layer. The filter take as input an array with shape (batch_size, Width, Channel),
compute a convolution using one or many filter and return a tensor with size (batch_size, new_Width, n_filters).
The Width can change regarding the filter_width, the stride and the padding selected.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>filter_width</strong> (<em>int</em>) – Width of the filter apply on the matrix.</li>
<li><strong>n_filter</strong> (<em>int</em>) – Number of filter of the convolution.</li>
<li><strong>stride</strong> (<em>int</em>) – Stride for the filter moving.</li>
<li><strong>padding</strong> (<em>str</em>) – Padding can be SAME or VALID. If PADDING is SAME the convolution output a matrix having the same size
as the input tensor. VALID keep the dimension reduction.</li>
<li><strong>add_bias</strong> (<em>bool</em>) – Whether to add the biase or not.</li>
<li><strong>act_funct</strong> (<em>str</em><em>, </em><em>None</em>) – Name of the activation function to use. If None no activation function are used.</li>
<li><strong>keep_proba</strong> (<em>float</em><em>, </em><em>tf.Tensor</em>) – Probability to keep a neuron activate during training if we want to apply the dropout method.</li>
<li><strong>batch_norm</strong> (<em>bool</em>) – If True apply the batch normalization after the _operator methods.</li>
<li><strong>batch_renorm</strong> (<em>bool</em>) – Whether to used batch renormalization or not.</li>
<li><strong>is_training</strong> (<em>tf.Tensor</em><em>, </em><em>None</em>) – Tensor indicating if data are used for training or to make prediction. Useful for batch normalization.</li>
<li><strong>name</strong> (<em>str</em>) – Name of the layer.</li>
<li><strong>law_name</strong> (<em>str</em>) – Name of the lax to use for weights and biases initialization.</li>
<li><strong>law_param</strong> (<em>float</em>) – Parameter of the initialization law.</li>
<li><strong>decay</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – Decay used to update the moving average of the batch norm. The moving average is used to learn the
empirical mean and variance of the output layer. It is recommended to set this value between (0.9, 1.).</li>
<li><strong>epsilon</strong> (<em>float</em>) – Parameters used to avoid infinity problem when scaling the output layer during the batch normalization.</li>
<li><strong>decay_renorm</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – Decay used to update by moving average the mu and sigma parameters when batch renormalization is used.</li>
<li><strong>rmin</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – Minimum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>rmax</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – Maximum ratio used to clip the standard deviation ratio when batch renormalization is applied.</li>
<li><strong>dmax</strong> (<em>tf.Tensor</em><em>, </em><em>float</em>) – When batch renormalization is used the scaled mu differences is clipped between (-dmax, dmax).</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>w</strong> (<em>Variable with size</em><em> (</em><em>width</em><em>, </em><em>n_channel</em><em>, </em><em>n_filter</em><em>)</em>) – Weight of the filter.</li>
<li><strong>b</strong> (<em>Variable with size</em><em> (</em><em>n_filter</em><em>,</em><em>)</em>) – Bias of the convolution.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.deep_learning.layer.Conv1dLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>x</em>, <em>w_init=None</em>, <em>b_init=None</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.Conv1dLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Allow to build the convolution taking in entry the x_input tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>x</strong> (<em>tf.Tensor</em>) – Input 3 dimensional tensor having the format ‘NWC’.</li>
<li><strong>w_init</strong> (<em>np.array with shape</em><em> (</em><em>width</em><em>, </em><em>n_channel</em><em>, </em><em>n_filter</em><em>)</em><em>, </em><em>None</em>) – Array to initialize the weight variable.</li>
<li><strong>b_init</strong> (<em>np.array with shape</em><em> (</em><em>n_filter</em><em>,</em><em>)</em><em>, </em><em>None</em>) – Array to initialize bias.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">Layer output Tensor.</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">tf.Tensor</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.layer.Conv1dLayer.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.Conv1dLayer.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore input/output tensor and all layer variables.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="core.deep_learning.layer.MinMaxLayer">
<em class="property">class </em><code class="descclassname">core.deep_learning.layer.</code><code class="descname">MinMaxLayer</code><span class="sig-paren">(</span><em>n_entries</em>, <em>name='minmax'</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.MinMaxLayer" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#core.deep_learning.abstract_operator.AbstractLayer" title="core.deep_learning.abstract_operator.AbstractLayer"><code class="xref py py-class docutils literal notranslate"><span class="pre">core.deep_learning.abstract_operator.AbstractLayer</span></code></a></p>
<p>Allow to use a MinMax layer. Given a 2 dimensional input array, this layer keep only the n best and the n
worse entries. The aim is to reduce the problem dimensionality by keeping only extremes values from the
input data.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>n_entries</strong> (<em>int</em>) – Number of top and worse neurons to keep.</li>
<li><strong>name</strong> (<em>str</em>) – Layer name.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="core.deep_learning.layer.MinMaxLayer.build">
<code class="descname">build</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.MinMaxLayer.build" title="Permalink to this definition">¶</a></dt>
<dd><p>Build the MinMax layer using the 2 dimensional input tensor x.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>x</strong> (<em>tf.Tensor</em>) – Input tensor filtered by the layer.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Layer output Tensor.</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">tf.Tensor</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="core.deep_learning.layer.MinMaxLayer.restore">
<code class="descname">restore</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#core.deep_learning.layer.MinMaxLayer.restore" title="Permalink to this definition">¶</a></dt>
<dd><p>Restore input/output tensor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><code class="docutils literal notranslate"><span class="pre">None</span></code></td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></li>
<li><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></li>
<li><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></li>
</ul>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019, Nicolas de Rémacle

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>